{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title pip install\n",
        "!pip install ipywidgets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yjt5v9rpov0b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import random\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Button, VBox, HBox, HTML, Layout"
      ],
      "metadata": {
        "id": "VrUEOiAsoxJm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Scenarios\n",
        "scenarios = [\n",
        "    # SIMPLE RULE scenarios - clear principles apply\n",
        "    {\n",
        "        \"text\": \"Your hiring algorithm shows 95% accuracy for all demographic groups. Legal team asks: 'Is this fair?'\",\n",
        "        \"is_complex\": False,\n",
        "        \"simple_rule\": \"Equal performance across groups = fair\",\n",
        "        \"explanation\": \"When accuracy is equal across groups, this is straightforward fairness. Simple rule applies.\",\n",
        "        \"complexity_trap\": \"Some might overthink this, but equal performance IS the goal.\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Algorithm uses protected characteristics (race, gender) directly as input features for loan decisions.\",\n",
        "        \"is_complex\": False,\n",
        "        \"simple_rule\": \"Never use protected characteristics directly\",\n",
        "        \"explanation\": \"Direct use of protected characteristics is clearly discriminatory. Simple rule applies.\",\n",
        "        \"complexity_trap\": \"No need for complex analysis - this is straightforward illegal discrimination.\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"AI system randomly assigns loan approvals regardless of any applicant information.\",\n",
        "        \"is_complex\": False,\n",
        "        \"simple_rule\": \"Random decisions are fair but useless\",\n",
        "        \"explanation\": \"Pure randomness is technically fair but completely defeats the purpose. Simple rule applies.\",\n",
        "        \"complexity_trap\": \"While random is 'fair,' it's obviously not a viable business solution.\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Facial recognition works perfectly for everyone regardless of age, race, or gender.\",\n",
        "        \"is_complex\": False,\n",
        "        \"simple_rule\": \"Equal performance = no bias problem\",\n",
        "        \"explanation\": \"If performance is truly equal across all groups, there's no bias issue. Simple rule applies.\",\n",
        "        \"complexity_trap\": \"Don't overcomplicate success - equal performance means the system is working fairly.\"\n",
        "    },\n",
        "\n",
        "    # COMPLEX REASONING scenarios - competing values, trade-offs, stakeholder conflicts\n",
        "    {\n",
        "        \"text\": \"Hiring algorithm: 85% accuracy for men, 82% accuracy for women. Legal says 'fix it,' Engineering says 'close enough,' HR says 'focus on candidate experience.'\",\n",
        "        \"is_complex\": True,\n",
        "        \"simple_rule\": None,\n",
        "        \"explanation\": \"Competing stakeholder priorities, unclear fairness standards, and trade-offs between accuracy and equity require complex reasoning.\",\n",
        "        \"complexity_factors\": [\"Multiple stakeholder views\", \"Unclear fairness threshold\", \"Accuracy vs. equity trade-off\"]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Medical AI: 90% accuracy for common diseases, 70% for rare diseases. Rare disease patients are predominantly from minority communities.\",\n",
        "        \"is_complex\": True,\n",
        "        \"simple_rule\": None,\n",
        "        \"explanation\": \"Medical necessity vs. equity, technical limitations vs. fairness obligations, and intersectional impacts require complex reasoning.\",\n",
        "        \"complexity_factors\": [\"Medical vs. social priorities\", \"Technical constraints\", \"Intersectional impacts\"]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Loan algorithm has equal accuracy across racial groups but approves fewer loans in low-income neighborhoods (which are predominantly minority).\",\n",
        "        \"is_complex\": True,\n",
        "        \"simple_rule\": None,\n",
        "        \"explanation\": \"Equal treatment vs. equal outcomes, individual vs. community impacts, and historical context require complex reasoning.\",\n",
        "        \"complexity_factors\": [\"Individual vs. group fairness\", \"Historical context\", \"Economic vs. social factors\"]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Your 'fair' algorithm reduces bias but also reduces overall hiring by 30%, potentially hurting company competitiveness.\",\n",
        "        \"is_complex\": True,\n",
        "        \"simple_rule\": None,\n",
        "        \"explanation\": \"Fairness vs. business viability, short-term vs. long-term impacts, and stakeholder trade-offs require complex reasoning.\",\n",
        "        \"complexity_factors\": [\"Business vs. ethical priorities\", \"Short vs. long-term thinking\", \"Multiple stakeholder impacts\"]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "T8eFM6IVym0f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the fairness pattern recognition training\n",
        "trainer = FairnessPatternTrainer()\n",
        "widget = trainer.start_training(15)  # Practice with 15 scenarios\n",
        "display(widget)"
      ],
      "metadata": {
        "id": "eYt-dV_OpEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V68d_YQDrW4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}